{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# %%\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    # 输入特征维度，输出特征维度 ，dropout概率，LeakyReLU 的负斜率，标志位\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        # 两个可学习参数\n",
    "        self.W = nn.Parameter(torch.zeros(in_features, out_features))\n",
    "        self.a = nn.Parameter(torch.zeros(2 * out_features, 1))\n",
    "        #  Xavier 初始化方法\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        # 初始化了一个 LeakyReLU 激活函数\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        '''\n",
    "        h: (N, in_features)\n",
    "        adj: sparse matrix with shape (N, N)\n",
    "        p\n",
    "        '''\n",
    "        adj=torch.squeeze(adj,-1)\n",
    "        # print(h.dtype)\n",
    "        # print(h.shape)\n",
    "\n",
    "        # 对h线性变换，得到中间特征矩阵Wh\n",
    "        Wh = torch.matmul(h, self.W)  # (N, out_features)\n",
    "\n",
    "       #  分别对矩阵 Wh 的不同部分进行线性变换，得到注意力权重矩阵 Wh1 和 Wh2。\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])  # (N, 1)\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])  # (N, 1)\n",
    "        # print(Wh1.shape)\n",
    "        # print(Wh2.shape)\n",
    "\n",
    "        # 通过将 Wh1 和 Wh2 进行转置，并相加得到注意力系数 e\n",
    "        # Wh1 + Wh2.T 是N*N矩阵，第i行第j列是Wh1[i]+Wh2[j]\n",
    "        # 那么Wh1 + Wh2.T的第i行第j列刚好就是文中的a^T*[Whi||Whj]\n",
    "        # 代表着节点i对节点j的attention\n",
    "        # print(torch.transpose(Wh2,2,1).shape)\n",
    "        e = self.leakyrelu(Wh1 +torch.transpose(Wh2,2,1))  # (N, N)\n",
    "        padding = (-2 ** 31) * torch.ones_like(e)  # (N, N)\n",
    "        # print(adj.shape)\n",
    "        # print(padding.shape)\n",
    "        \n",
    "        # 根据邻接矩阵 adj 及其与注意力系数 e 的比较，将无效的注意力系数置为负无穷大\n",
    "        attention = torch.where(adj > 0, e, padding)  # (N, N)\n",
    "        # 对有效的注意力系数进行 softmax 归一化\n",
    "        attention = F.softmax(attention, dim=1)  # (N, N)\n",
    "        # attention矩阵第i行第j列代表node_i对node_j的注意力\n",
    "        # 对注意力权重也做dropout（如果经过mask之后，attention矩阵也许是高度稀疏的，这样做还有必要吗？）\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "\n",
    "        # 注意力系数加权求和  \n",
    "        h_prime = torch.matmul(attention, Wh)  # (N, out_features)\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self,date_emb, nfeat, nhid, dropout, alpha, nheads):\n",
    "        super(GAT, self).__init__()\n",
    "        date_index_number,date_dim = date_emb[0], date_emb[1]\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 输入特征维度 nfeat，隐藏层维度 nhid，Dropout 概率 dropout，LeakyReLU 的负斜率 alpha，以及注意力头数 nheads\n",
    "        self.MH = nn.ModuleList([\n",
    "            GraphAttentionLayer(nfeat, nhid, dropout, alpha, concat=True)\n",
    "            for _ in range(nheads)\n",
    "        ])\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nhid, dropout, alpha, concat=False)\n",
    "        self.date_embdding = nn.Embedding(date_index_number,date_dim)\n",
    "        self.active_index = nn.Linear(nhid,1)\n",
    "        self.consume_index = nn.Linear(nhid,1)\n",
    "\n",
    "        \n",
    "    def forward(self,x_date,x_feature,x_mask_data):\n",
    "\n",
    "\n",
    "        x = x_feature\n",
    "        # x = F.dropout(x_feature, self.dropout, training=self.training)  # (N, nfeat)\n",
    "        x = torch.cat([head(x, x_mask_data) for head in self.MH], dim=-1)  # (N, nheads*nhid)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)  # (N, nfeat)\n",
    "\n",
    "\n",
    "        # x = F.dropout(x, self.dropout, training=self.training)  # (N, nheads*nhid)\n",
    "        x = self.out_att(x, x_mask_data)\n",
    "        # print(x.shape,x.dtype)\n",
    "        act_pre= self.active_index(x)\n",
    "        con_pre = self.consume_index(x)\n",
    "        return  act_pre,con_pre\n",
    "\n",
    "\n",
    "class BILSTM(nn.Module):\n",
    "    def __init__(self,date_emb, nfeat, nhid, dropout, alpha, nheads):\n",
    "        super(BILSTM, self).__init__()\n",
    "        date_index_number,date_dim = date_emb[0], date_emb[1]\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(nfeat,\n",
    "                nhid,\n",
    "                num_layers=2,\n",
    "                bias=True,\n",
    "                batch_first=False,\n",
    "                dropout=0,\n",
    "                bidirectional=True)\n",
    "\n",
    "        self.active_index = nn.Linear(2*nhid, 1)\n",
    "        self.consume_index = nn.Linear(2*nhid, 1)\n",
    "    def forward(self,x_date,x_feature,x_mask_data):\n",
    "        lstm_out, (hidden, cell) = self.lstm(x_feature)\n",
    "        x = lstm_out\n",
    "        # print(x.shape)\n",
    "\n",
    "\n",
    "        x = F.dropout(x, self.dropout, training=self.training)  # (N, nheads*nhid)\n",
    "        act_pre= self.active_index(x)\n",
    "        con_pre = self.consume_index(x)\n",
    "        # print(act_pre.shape,con_pre.shape)\n",
    "        return  act_pre,con_pre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3-pytorch]",
   "language": "python",
   "name": "conda-env-Anaconda3-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
