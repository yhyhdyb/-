{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 读取CSV文件\n",
    "import my_model\n",
    "import  torch\n",
    "import argparse\n",
    "from tqdm import tqdm, trange\n",
    "import data\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()  # 使用均方误差损失函数计算MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得训练集\n",
    "def get_train_data(file_path,edge_pth):\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    edge_df = pd.read_csv(edge_pth, encoding='utf-8')\n",
    "    df.head()\n",
    "    geohasd_df_dict = {}\n",
    "    date_df_dict = {}\n",
    "    number_hash = 0\n",
    "    number_date = 0\n",
    "    for i in df[\"geohash_id\"]:\n",
    "\n",
    "        if i not in geohasd_df_dict.keys():\n",
    "            geohasd_df_dict[i] = number_hash\n",
    "            number_hash += 1\n",
    "\n",
    "    for i in df[\"date_id\"]:\n",
    "        if i not in date_df_dict.keys():\n",
    "            date_df_dict[i] = number_date\n",
    "            number_date += 1\n",
    "\n",
    "    # new_data 的二维列表。它的大小是 len(date_df_dict) 行乘以 len(geohasd_df_dict) 列。\n",
    "    # 具体地，代码首先使用 geohasd_df_dict 将地理哈希编码转换为 new_data 的列索引，\n",
    "    # 使用 date_df_dict 将日期转换为 new_data 的行索引，\n",
    "    # 然后将新数据填入 new_data 对应位置的单元格中。\n",
    "    # 这可以使用两个索引进行操作：date_index （行）和 hash_index （列），\n",
    "    # 它们是通过哈希表存储的，因此读取非常快。\n",
    "    new_data = [len(geohasd_df_dict) * [0]] * len(date_df_dict)\n",
    "    for index, row in df.iterrows():\n",
    "#         print(index)\n",
    "        hash_index, date_index = geohasd_df_dict[row[\"geohash_id\"]], date_df_dict[row[\"date_id\"]]\n",
    "        #将时间index加到里面\n",
    "\n",
    "        new_data[date_index][hash_index] = [date_index]+list(row.iloc[2:])\n",
    "    new_data = np.array(new_data)\n",
    "    # x_train,y_train = new_data[:, :-2], new_data[:, -2:]\n",
    "    # print(len(geohasd_df_dict))\n",
    "    # exit()\n",
    "    # print(x_train.shape)\n",
    "    # print(y_train.shape)\n",
    "    #这里构建邻接矩阵其中mask表示1为有边，0无边， value_mask表示有值\n",
    "    #并且这里我考虑mask是一个无向图，如果有向删除x_mask[date_index][point2_index][point1_index],value_mask同理\n",
    "   \n",
    "    x_mask =  np.zeros((len(date_df_dict),len(geohasd_df_dict),len(geohasd_df_dict),1), dtype = float)\n",
    "    x_edge_df =np.zeros((len(date_df_dict),len(geohasd_df_dict),len(geohasd_df_dict),2), dtype = float)\n",
    "\n",
    "    for index, row in edge_df.iterrows():\n",
    "        # print(index)\n",
    "        if row[\"geohash6_point1\"] not in geohasd_df_dict.keys() or row[\"geohash6_point2\"] not in geohasd_df_dict.keys():\n",
    "            continue\n",
    "        point1_index,point2_index,F_1,F_2,date_index= geohasd_df_dict[row[\"geohash6_point1\"]],geohasd_df_dict[row[\"geohash6_point2\"]]\\\n",
    "            ,row[\"F_1\"],row[\"F_2\"],date_df_dict[row[\"date_id\"]]\n",
    "        \n",
    "        # 将 x_mask 中对应位置的值设为1，表示该位置存在边。\n",
    "        x_mask[date_index][point1_index][point2_index] = 1\n",
    "        x_mask[date_index][point2_index][point1_index] = 1\n",
    "\n",
    "        # 同时，将 x_edge_df 中对应位置的值设为 [F_1, F_2]，即该位置的特征向量。\n",
    "        x_edge_df[date_index][point1_index][point2_index] =  [F_1,F_2]\n",
    "        x_edge_df[date_index][point2_index][point1_index] = [F_1, F_2]\n",
    "    # print(data)\n",
    "\n",
    "    return     geohasd_df_dict, date_df_dict, new_data,x_mask, x_edge_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataset, args):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        dev_loss = 0.0\n",
    "        for j in trange(dataset.batch_count):\n",
    "            x_date, x_feature, x_mask_data, x_edge_data, x_tags = dataset.get_batch(j)\n",
    "            act_pre, con_pre = model(x_date, x_feature, x_mask_data)\n",
    "            predict = torch.cat((act_pre, con_pre), dim=-1)\n",
    "            loss = criterion(predict, x_tags)\n",
    "            dev_loss+= loss\n",
    "        print(\"this epoch dev loss is {}\".format(dev_loss))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "\n",
    "    #使用get_train_data函数获取训练数据和标签。\n",
    "    geohasd_df_dict, date_df_dict, x_train, x_mask, x_edge_df = get_train_data('./train_90.csv',\n",
    "                                                                                        \"./edge_90.csv\")\n",
    "    #分割各种训练集测试集\n",
    "    x_train,x_dev = torch.tensor(x_train[:int(len(x_train)*args.rat)]),torch.tensor(x_train[int(len(x_train)*args.rat):])\n",
    "    x_mask_train,x_mask_dev = torch.tensor(x_mask[:int(len(x_mask)*args.rat)]),torch.tensor(x_mask[int(len(x_mask)*args.rat):])\n",
    "    x_edge_train, x_edge_dev = torch.tensor(x_edge_df[:int(len(x_edge_df) * args.rat)]),torch.tensor( x_edge_df[int(len(x_edge_df) * args.rat):])\n",
    "\n",
    "    date_emb = 5\n",
    "     # 这里的x包含了date_id+F35个特征+2个y值的\n",
    "    # train_activate = torch.tensor(y_train[:, -2])\n",
    "    # train_consume = torch.tensor(y_train[:, -1])\n",
    "\n",
    "    # rmse_loss = torch.sqrt(mse_loss)\n",
    "\n",
    "    #定义模型并将其移动到指定设备上。\n",
    "    model = my_model.GAT(date_emb =[len(date_df_dict),date_emb], nfeat=35, nhid=64, dropout=0.3, alpha=0.3, nheads=8).to(args.device)\n",
    "    # model = my_model.BILSTM(date_emb =[len(date_df_dict),date_emb], nfeat=35, nhid=64, dropout=0.3, alpha=0.3, nheads=8).to(args.device)\n",
    "    \n",
    "    #定义优化器（使用Adam算法）和损失函数（criterion）\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),lr=args.lr)\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.decline, gamma=0.5, last_epoch=-1)\n",
    "    model.train()\n",
    "\n",
    "    #创建训练集和验证集的数据迭代器。\n",
    "    trainset = data.DataIterator(x_train,x_mask_train,x_edge_train, args)\n",
    "    valset =data.DataIterator(x_dev,x_mask_dev,x_edge_dev, args)\n",
    "    for indx in range(args.epochs):\n",
    "        train_all_loss = 0.0\n",
    "        for j in trange(trainset.batch_count):\n",
    "            #获取当前batch的数据（日期、特征、掩码、边缘数据和标签）\n",
    "            x_date,x_feature,x_mask_data,x_edge_data,x_tags= trainset.get_batch(j)\n",
    "            act_pre, con_pre = model(x_date,x_feature,x_mask_data)\n",
    "            predict = torch.cat((act_pre, con_pre), dim=-1)\n",
    "\n",
    "            #计算预测结果和标签之间的损失。\n",
    "            loss = criterion(predict, x_tags)\n",
    "            train_all_loss += loss\n",
    "\n",
    "            #清除优化器的梯度。\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #反向传播并更新模型的参数。\n",
    "            loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        print('this epoch train loss :{0}'.format(train_all_loss))\n",
    "        # scheduler.step()\n",
    "\n",
    "        #调用eval函数对验证集进行评估。\n",
    "        eval(model,valset, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--epochs EPOCHS] [--batch_size BATCH_SIZE]\n",
      "                             [--device DEVICE] [--lr LR] [--rat RAT]\n",
      "                             [--decline DECLINE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\JIN\\AppData\\Roaming\\jupyter\\runtime\\kernel-7656994f-ef12-4172-a330-eef24ee44476.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--epochs', type=int, default=300,\n",
    "                        help='training epoch number')\n",
    "    parser.add_argument('--batch_size', type=int, default=4,\n",
    "                        help='batch_size')\n",
    "    parser.add_argument('--device', type=str, default=\"cuda\",\n",
    "                        help='gpu or cpu')\n",
    "    parser.add_argument('--lr', type=float, default=1e-3,\n",
    "                        )\n",
    "    parser.add_argument('--rat', type=float, default=0.9,)\n",
    "\n",
    "    parser.add_argument('--decline', type=int, default=30, help=\"number of epochs to decline\")\n",
    "    \n",
    "    train(parser.parse_args())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_49484\\3848345139.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--decline'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"number of epochs to decline\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\JIN\\Anaconda3\\envs\\pytorch\\lib\\argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1757\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unrecognized arguments: %s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1758\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1759\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\JIN\\Anaconda3\\envs\\pytorch\\lib\\argparse.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2505\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2506\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'prog'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'message'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2507\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\JIN\\Anaconda3\\envs\\pytorch\\lib\\argparse.py\u001b[0m in \u001b[0;36mexit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2492\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2493\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2494\u001b[1;33m         \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2496\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GCN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_49484\\492793093.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgcn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGCN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m51\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'GCN' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "gcn = GCN(n_features=51, hidden_dim=64, dropout=0.3, n_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# date_node = np.concatenate([sample.iloc[:, 2:37].values, date_embed], axis=1)\n",
    "# date_node.shape\n",
    "#\n",
    "# # %%\n",
    "#\n",
    "# adj = torch.randn(90, 90)\n",
    "# x_node = torch.from_numpy(date_node).type_as(adj)\n",
    "# predict = gcn(x_node, adj)\n",
    "# predict.shape, predict\n",
    "#\n",
    "# # %%\n",
    "#\n",
    "# gat = GAT(nfeat=51, nhid=64, nclass=2, dropout=0.3, alpha=0.3, nheads=8)\n",
    "#\n",
    "# # %%\n",
    "#\n",
    "# adj = torch.randn(90, 90)\n",
    "# x_node = torch.from_numpy(date_node).type_as(adj)\n",
    "# predict = gat(x_node, adj)\n",
    "# predict.shape, predict\n",
    "#\n",
    "# # %%\n",
    "#\n",
    "# actual_values = sample.iloc[:, 37:]  # active_index，consume_index\n",
    "#\n",
    "# # %%\n",
    "#\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "#\n",
    "# # 定义预测值和实际观测值\n",
    "# actual_values = torch.from_numpy(sample.iloc[:, 37:].values)  # 实际观测值\n",
    "#\n",
    "# # 计算RMSE损失\n",
    "# criterion = nn.MSELoss()  # 使用均方误差损失函数计算MSE\n",
    "# mse_loss = criterion(predict, actual_values)\n",
    "# rmse_loss = torch.sqrt(mse_loss)\n",
    "#\n",
    "# # 打印RMSE损失\n",
    "# print(\"RMSE Loss:\", rmse_loss.item())\n",
    "#\n",
    "# # %%\n",
    "#\n",
    "# x_node, adj\n",
    "#\n",
    "# # %%\n",
    "#\n",
    "# import pandas as pd\n",
    "#\n",
    "# # 读取CSV文件\n",
    "# edge = pd.read_csv('./edge_90.csv')\n",
    "#\n",
    "# # %%\n",
    "#\n",
    "# edge.head()\n",
    "#\n",
    "# # %%\n",
    "#\n",
    "# geohash_id = df.geohash_id\n",
    "#\n",
    "# # %%\n",
    "#\n",
    "# geohash_id.head()\n",
    "#\n",
    "# # %%\n",
    "#\n",
    "# align = edge[(edge['geohash6_point1'] == df.geohash_id[0]) & (edge['date_id'] == df.date_id[0])]\n",
    "#\n",
    "# # %%\n",
    "#\n",
    "# align\n",
    "#\n",
    "# # %%\n",
    "#\n",
    "# df.geohash_id.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3-pytorch-2]",
   "language": "python",
   "name": "conda-env-Anaconda3-pytorch-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
