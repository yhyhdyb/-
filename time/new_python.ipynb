{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d54c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 读取CSV文件\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# tqdm函数用于为循环或迭代器创建进度条。\n",
    "# 它可以用于显示需要很长时间才能完成的任务的进度，例如数据处理或模型训练。\n",
    "# trange函数类似于Python中的range函数，但它还创建了有指定迭代次数的进度条。\n",
    "# 这允许您实时查看循环的进度，因此可以更好地跟踪任务执行情况。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51654725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my_model   # my_model是自己写的一个文件\n",
    "# import data # data是一个自己写的文件\n",
    "import torch.nn as nn\n",
    "\"\"\"\n",
    "这里导入了pytorch深度学习的nn模块，这个模块提供了神经网络层、损失函数\n",
    "和优化器等工具的类，通过这个模块可以方便地构建和训练神经网络模型。\n",
    "\"\"\"\n",
    "min_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dea19016",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()  # 使用均方误差损失函数计算MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e54d138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import  pickle\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97d387ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIterator(object):\n",
    "    def __init__(self, x_data,x_mask_data,x_edge_data, args):\n",
    "        self.x_data,self.x_mask_data,self.x_edge_data,=x_data,x_mask_data,x_edge_data,\n",
    "        #date跟fearture的分开\n",
    "        # 虽然x_date是一个二维数组，但是二维数组中的每个元素都是一个列表，每个列表的内容是一个日期id和三十五个F特征还有要预测的两个目标值\n",
    "        # x_data[:,:,0]取出来的是日期\n",
    "        # x_data[:,:,1:-2]取出来的是三十五个特征\n",
    "        # x_data[:,:,-2:]取出来的是两个预测值也就是\n",
    "        self.x_date,self.x_feature,self.x_tags=self.x_data[:,:,0],self.x_data[:,:,1:-2],x_data[:,:,-2:]\n",
    "        # print(self.x_date.shape,self.x_feature.shape,self.x_tags.shape)\n",
    "        self.args = args\n",
    "        #通过数据总数除掉每个批次的数据数目args.batch_size来算出一共多少个批次\n",
    "        self.batch_count = math.ceil(len(x_data)/args.batch_size)\n",
    "\n",
    "        #get_batch函数是用来获取某个训练批次的数据的，index代表批次号\n",
    "    def get_batch(self, index):\n",
    "        x_date = []\n",
    "        x_feature = []\n",
    "        x_mask_data=[]\n",
    "        x_edge_data = []\n",
    "        x_tags = []\n",
    "\n",
    "\n",
    "        for i in range(index * self.args.batch_size,\n",
    "                       min((index + 1) * self.args.batch_size, len(self.x_data))):\n",
    "\n",
    "            x_date.append(self.x_date[i])\n",
    "            x_feature.append(self.x_feature[i].float() )\n",
    "\n",
    "            # print(self.x_mask_data[i].shape)\n",
    "            x_mask_data.append(self.x_mask_data[i])\n",
    "            # print(self.x_edge_data[i].shape)\n",
    "            x_edge_data.append(self.x_edge_data[i])\n",
    "            x_tags.append(self.x_tags[i].float() )\n",
    "\n",
    "        x_date = torch.stack(x_date).to(self.args.device)\n",
    "        x_feature = torch.FloatTensor(torch.stack(x_feature)).to(self.args.device)\n",
    "        x_mask_data = torch.stack(x_mask_data).to(self.args.device)\n",
    "        x_edge_data = torch.stack(x_edge_data).to(self.args.device)\n",
    "        x_tags = torch.stack(x_tags).to(self.args.device)\n",
    "\n",
    "\n",
    "        return  x_date,x_feature,x_mask_data,x_edge_data,x_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6286b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07fe3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout, bias=False):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)  # xavier初始化，就是论文里的glorot初始化\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "            nn.init.zeros_(self.bias)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, inputs, adj):\n",
    "        # inputs: (N, n_channels), adj: sparse_matrix (N, N)\n",
    "        support = torch.mm(self.dropout(inputs), self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f60ec6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim, dropout, n_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(n_features, hidden_dim, dropout)\n",
    "        self.gc2 = GraphConvolution(hidden_dim, n_classes, dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs, adj):\n",
    "        x = inputs\n",
    "        x = self.relu(self.gc1(x, adj))\n",
    "        x = self.gc2(x, adj)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce3e6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08dca4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(in_features, out_features))\n",
    "        self.a = nn.Parameter(torch.zeros(2 * out_features, 1))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        '''\n",
    "        h: (N, in_features)\n",
    "        adj: sparse matrix with shape (N, N)\n",
    "        p\n",
    "        '''\n",
    "        adj=torch.squeeze(adj,-1)\n",
    "        # print(h.dtype)\n",
    "        # print(h.shape)\n",
    "        h = h.type_as(self.W)\n",
    "        Wh = torch.matmul(h, self.W)  # (N, out_features)\n",
    "        #print(h)\n",
    "        #print(self.W)\n",
    "\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])  # (N, 1)\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])  # (N, 1)\n",
    "        # print(Wh1.shape)\n",
    "        # print(Wh2.shape)\n",
    "\n",
    "        # Wh1 + Wh2.T 是N*N矩阵，第i行第j列是Wh1[i]+Wh2[j]\n",
    "        # 那么Wh1 + Wh2.T的第i行第j列刚好就是文中的a^T*[Whi||Whj]\n",
    "        # 代表着节点i对节点j的attention\n",
    "        # print(torch.transpose(Wh2,2,1).shape)\n",
    "        e = self.leakyrelu(Wh1 +torch.transpose(Wh2,2,1))  # (N, N)\n",
    "        padding = (-2 ** 31) * torch.ones_like(e)  # (N, N)\n",
    "        # print(adj.shape)\n",
    "        # print(padding.shape)\n",
    "        attention = torch.where(adj > 0, e, padding)  # (N, N)\n",
    "        attention = F.softmax(attention, dim=1)  # (N, N)\n",
    "        # attention矩阵第i行第j列代表node_i对node_j的注意力\n",
    "        # 对注意力权重也做dropout（如果经过mask之后，attention矩阵也许是高度稀疏的，这样做还有必要吗？）\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "\n",
    "        h_prime = torch.matmul(attention, Wh)  # (N, out_features)\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44813ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self,date_emb, nfeat, nhid, dropout, alpha, nheads):\n",
    "        super(GAT, self).__init__()\n",
    "        date_index_number,date_dim = date_emb[0], date_emb[1]\n",
    "        self.dropout = dropout\n",
    "        self.MH = nn.ModuleList([\n",
    "            GraphAttentionLayer(nfeat, nhid, dropout, alpha, concat=True)\n",
    "            for _ in range(nheads)\n",
    "        ])\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nhid, dropout, alpha, concat=False)\n",
    "        self.date_embdding = nn.Embedding(date_index_number,date_dim)\n",
    "        self.active_index = nn.Linear(nhid,1)\n",
    "        self.consume_index = nn.Linear(nhid,1)\n",
    "    def forward(self,x_date,x_feature,x_mask_data):\n",
    "\n",
    "\n",
    "        x = x_feature\n",
    "        # x = F.dropout(x_feature, self.dropout, training=self.training)  # (N, nfeat)\n",
    "        x = torch.cat([head(x, x_mask_data) for head in self.MH], dim=-1)  # (N, nheads*nhid)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)  # (N, nfeat)\n",
    "\n",
    "\n",
    "        # x = F.dropout(x, self.dropout, training=self.training)  # (N, nheads*nhid)\n",
    "        x = self.out_att(x, x_mask_data)\n",
    "        # print(x.shape,x.dtype)\n",
    "        act_pre= self.active_index(x)\n",
    "        con_pre = self.consume_index(x)\n",
    "        return  act_pre,con_pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e39e4ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BILSTM(nn.Module):\n",
    "    def __init__(self,date_emb, nfeat, nhid, dropout, alpha, nheads):\n",
    "        super(BILSTM, self).__init__()\n",
    "        date_index_number,date_dim = date_emb[0], date_emb[1]\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(nfeat,\n",
    "                nhid,\n",
    "                num_layers=2,\n",
    "                bias=True,\n",
    "                batch_first=False,\n",
    "                dropout=0,\n",
    "                bidirectional=True)\n",
    "\n",
    "        self.active_index = nn.Linear(2*nhid, 1)\n",
    "        self.consume_index = nn.Linear(2*nhid, 1)\n",
    "    def forward(self,x_date,x_feature,x_mask_data):\n",
    "        lstm_out, (hidden, cell) = self.lstm(x_feature)\n",
    "        x = lstm_out\n",
    "        # print(x.shape)\n",
    "\n",
    "\n",
    "        x = F.dropout(x, self.dropout, training=self.training)  # (N, nheads*nhid)\n",
    "        act_pre= self.active_index(x)\n",
    "        con_pre = self.consume_index(x)\n",
    "        # print(act_pre.shape,con_pre.shape)\n",
    "        return  act_pre,con_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1401894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd2df9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(file_path,edge_pth):\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    #读取图文件，并且把它存储为一个名字为df的对象，并且指定文件字符编码为UFT-8编码\n",
    "    edge_df = pd.read_csv(edge_pth, encoding='utf-8')\n",
    "    df.head()\n",
    "\n",
    "    # %%\n",
    "    # 先定义两个空字典变量，分别用来存储地点和日期和某个序号的映射\n",
    "    geohasd_df_dict = {}\n",
    "    date_df_dict = {}\n",
    "    #最开始字典为空，先声明两个初始值为0的变量\n",
    "    number_hash = 0\n",
    "    number_date = 0\n",
    "    \n",
    "    # 使用循环\n",
    "    for i in df[\"geohash_id\"]:\n",
    "\n",
    "        if i not in geohasd_df_dict.keys():\n",
    "            geohasd_df_dict[i] = number_hash\n",
    "            number_hash += 1 #字典已经存入了一个地点，那么number_hash就应该加1\n",
    "            \n",
    "    for i in df[\"date_id\"]:\n",
    "        if i not in date_df_dict.keys():\n",
    "            date_df_dict[i] = number_date\n",
    "            number_date += 1\n",
    "\n",
    "    # 这里创建了一个列表，行数就是日期的数目，列数是地点的数目\n",
    "    # 子列表数目就是\n",
    "    new_data = new_data = np.zeros((len(date_df_dict),len(geohasd_df_dict),38))\n",
    "    # [len(geohasd_df_dict) * [0]] * len(date_df_dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "    # iterrows()方法会返回一个元组，其中包含当前行的索引和数据\n",
    "    # index变量将保存当前行的索引，row变量将保存当前行的数据\n",
    "        # print(index)\n",
    "        hash_index, date_index = geohasd_df_dict[row[\"geohash_id\"]], date_df_dict[row[\"date_id\"]]\n",
    "        #将时间index加到里面\n",
    "        \"\"\"\n",
    "        这里new_data[date_index][hash_index]将被赋值为一个列表。\n",
    "        这个列表的第一个元素是date_index，后面是row.iloc[2:]的值。\n",
    "        row.iloc[2:]表示从row中的第三个元素开始到最后一个元素的切片。\n",
    "        包含一行中从第三列开始的所有列的值。\n",
    "        \"\"\"\n",
    "        new_data[date_index][hash_index] = [date_index]+list(row.iloc[2:])\n",
    "    new_data = np.array(new_data) \n",
    "    # 这里new_data转换为numpy数组，这个函数会创建一个具有相同维度和元素的新数组\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # x_train,y_train = new_data[:, :-2], new_data[:, -2:]\n",
    "    # print(len(geohasd_df_dict))\n",
    "    # exit()\n",
    "    # print(x_train.shape)\n",
    "    # print(y_train.shape)\n",
    "    #这里构建邻接矩阵其中mask表示1为有边，0无边， value_mask表示有值\n",
    "    #并且这里我考虑mask是一个无向图，如果有向删除x_mask[date_index][point2_index][point1_index],value_mask同理\n",
    "    \n",
    "    #下面两个是元素值均为0的数组\n",
    "    x_mask =  np.zeros((len(date_df_dict),len(geohasd_df_dict),len(geohasd_df_dict),1), dtype = float)\n",
    "    x_edge_df =np.zeros((len(date_df_dict),len(geohasd_df_dict),len(geohasd_df_dict),2), dtype = float)\n",
    "\n",
    "    for index, row in edge_df.iterrows():\n",
    "        # print(index)\n",
    "        # 地点编号在字典中找不到，就说明这个数据是错误的，出现了错误的地点，那就进入下一层循环\n",
    "        if row[\"geohash6_point1\"] not in geohasd_df_dict.keys() or row[\"geohash6_point2\"] not in geohasd_df_dict.keys():\n",
    "            continue\n",
    "        point1_index,point2_index,F_1,F_2,date_index= geohasd_df_dict[row[\"geohash6_point1\"]],geohasd_df_dict[row[\"geohash6_point2\"]]\\\n",
    "            ,row[\"F_1\"],row[\"F_2\"],date_df_dict[row[\"date_id\"]]\n",
    "        x_mask[date_index][point1_index][point2_index] = 1\n",
    "        x_mask[date_index][point2_index][point1_index] = 1\n",
    "        # 这里把mask数组对应位置赋值为1，说明两个地点之间有边\n",
    "        x_edge_df[date_index][point1_index][point2_index] =  [F_1,F_2]\n",
    "        x_edge_df[date_index][point2_index][point1_index] = [F_1, F_2]\n",
    "        # 把边上面的两个特征存入数组x_edge_df中\n",
    "    # print(data)\n",
    "\n",
    "    return     geohasd_df_dict, date_df_dict, new_data,x_mask, x_edge_df\n",
    "#get_train_data函数运行完之后，把地点、时间字典等各种数据做一个返回\n",
    "#newdata存的是某个日期某个地点的具体特征\n",
    "#x_mask 存的是某个日期两个地点之间是否有边\n",
    "#x_edge_df存的是边上的两个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbcecfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(file_path,edge_pth):\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    #读取图文件，并且把它存储为一个名字为df的对象，并且指定文件字符编码为UFT-8编码\n",
    "    edge_df = pd.read_csv(edge_pth, encoding='utf-8')\n",
    "    df.head()\n",
    "\n",
    "    # %%\n",
    "    # 先定义两个空字典变量，分别用来存储地点和日期和某个序号的映射\n",
    "    geohasd_df_dict = {}\n",
    "    date_df_dict = {}\n",
    "    #最开始字典为空，先声明两个初始值为0的变量\n",
    "    number_hash = 0\n",
    "    number_date = 0\n",
    "    \n",
    "    # 使用循环\n",
    "    for i in df[\"geohash_id\"]:\n",
    "\n",
    "        if i not in geohasd_df_dict.keys():\n",
    "            geohasd_df_dict[i] = number_hash\n",
    "            number_hash += 1 #字典已经存入了一个地点，那么number_hash就应该加1\n",
    "            \n",
    "    for i in df[\"date_id\"]:\n",
    "        if i not in date_df_dict.keys():\n",
    "            date_df_dict[i] = number_date\n",
    "            number_date += 1\n",
    "\n",
    "    # 这里创建了一个列表，行数就是日期的数目，列数是地点的数目\n",
    "    # 子列表数目就是\n",
    "    new_data = new_data = np.zeros((len(date_df_dict),len(geohasd_df_dict),36))\n",
    "    # [len(geohasd_df_dict) * [0]] * len(date_df_dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "    # iterrows()方法会返回一个元组，其中包含当前行的索引和数据\n",
    "    # index变量将保存当前行的索引，row变量将保存当前行的数据\n",
    "        # print(index)\n",
    "        hash_index, date_index = geohasd_df_dict[row[\"geohash_id\"]], date_df_dict[row[\"date_id\"]]\n",
    "        #将时间index加到里面\n",
    "        \"\"\"\n",
    "        这里new_data[date_index][hash_index]将被赋值为一个列表。\n",
    "        这个列表的第一个元素是date_index，后面是row.iloc[2:]的值。\n",
    "        row.iloc[2:]表示从row中的第三个元素开始到最后一个元素的切片。\n",
    "        包含一行中从第三列开始的所有列的值。\n",
    "        \"\"\"\n",
    "        new_data[date_index][hash_index] = [date_index]+list(row.iloc[2:])\n",
    "    new_data = np.array(new_data) \n",
    "    # 这里new_data转换为numpy数组，这个函数会创建一个具有相同维度和元素的新数组\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # x_train,y_train = new_data[:, :-2], new_data[:, -2:]\n",
    "    # print(len(geohasd_df_dict))\n",
    "    # exit()\n",
    "    # print(x_train.shape)\n",
    "    # print(y_train.shape)\n",
    "    #这里构建邻接矩阵其中mask表示1为有边，0无边， value_mask表示有值\n",
    "    #并且这里我考虑mask是一个无向图，如果有向删除x_mask[date_index][point2_index][point1_index],value_mask同理\n",
    "    \n",
    "    #下面两个是元素值均为0的数组\n",
    "    x_mask =  np.zeros((len(date_df_dict),len(geohasd_df_dict),len(geohasd_df_dict),1), dtype = float)\n",
    "    x_edge_df =np.zeros((len(date_df_dict),len(geohasd_df_dict),len(geohasd_df_dict),2), dtype = float)\n",
    "\n",
    "    for index, row in edge_df.iterrows():\n",
    "        # print(index)\n",
    "        # 地点编号在字典中找不到，就说明这个数据是错误的，出现了错误的地点，那就进入下一层循环\n",
    "        if row[\"geohash6_point1\"] not in geohasd_df_dict.keys() or row[\"geohash6_point2\"] not in geohasd_df_dict.keys():\n",
    "            continue\n",
    "        point1_index,point2_index,F_1,F_2,date_index= geohasd_df_dict[row[\"geohash6_point1\"]],geohasd_df_dict[row[\"geohash6_point2\"]]\\\n",
    "            ,row[\"F_1\"],row[\"F_2\"],date_df_dict[row[\"date_id\"]]\n",
    "        x_mask[date_index][point1_index][point2_index] = 1\n",
    "        x_mask[date_index][point2_index][point1_index] = 1\n",
    "        # 这里把mask数组对应位置赋值为1，说明两个地点之间有边\n",
    "        x_edge_df[date_index][point1_index][point2_index] =  [F_1,F_2]\n",
    "        x_edge_df[date_index][point2_index][point1_index] = [F_1, F_2]\n",
    "        # 把边上面的两个特征存入数组x_edge_df中\n",
    "    # print(data)\n",
    "\n",
    "    return     geohasd_df_dict, date_df_dict, new_data,x_mask, x_edge_df\n",
    "#get_train_data函数运行完之后，把地点、时间字典等各种数据做一个返回\n",
    "#newdata存的是某个日期某个地点的具体特征\n",
    "#x_mask 存的是某个日期两个地点之间是否有边\n",
    "#x_edge_df存的是边上的两个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbe3e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是一个评估函数，评估模型在给定数据集上的性能\n",
    "# model是要评估的模型\n",
    "# dataset是要用于评估模型的数据的数据集对象\n",
    "# args是包含其他辅助参数的对象\n",
    "def eval(model, dataset, args):\n",
    "    global min_loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        dev_loss = 0.0\n",
    "        for j in trange(dataset.batch_count):\n",
    "            x_date, x_feature, x_mask_data, x_edge_data, x_tags = dataset.get_batch(j)\n",
    "            act_pre, con_pre = model(x_date, x_feature, x_mask_data)\n",
    "            predict = torch.cat((act_pre, con_pre), dim=-1)\n",
    "            loss = criterion(predict, x_tags)\n",
    "            dev_loss+= loss\n",
    "        print(\"this epoch dev loss is {}\".format(dev_loss))\n",
    "        if  dev_loss < min_loss:\n",
    "            min_loss = dev_loss\n",
    "            # best_model_params = model.state_dict()\n",
    "            torch.save(model, 'best_model_{}.pth'.format(dev_loss))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f82dfdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "\n",
    "    #args是一个参数，它有两个特征args.rat还有args.batch_size\n",
    "    #args.rat代表要把文件里面的取出来多少百分比作为训练集\n",
    "    #args.batch_size代表一个批次要多少数据\n",
    "    \n",
    "    geohasd_df_dict, date_df_dict, x_train, x_mask, x_edge_df = get_train_data('./train_90.csv',\n",
    "                                                                                        \"./edge_90.csv\")\n",
    "    # 这里的x_train存的是各个地点及其身上的各个特征\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #分割各种训练集测试集\n",
    "    # 这里把存放着train_90.csv的矩阵x_train进行分割，分割成训练集和测试集\n",
    "    x_train,x_dev = torch.tensor(x_train[:int(len(x_train)*args.rat)]),torch.tensor(x_train[int(len(x_train)*args.rat):])\n",
    "    # x_train训练集取了前len(x_train)*args.rat天的数据，x_dev测试集取了后len(x_train)*args.rat天的数据。\n",
    "    \"\"\"\n",
    "    x_train[:int(len(x_train)*args.rat)]语句获取了 x_train 数据的前 args.rat 比例部分\n",
    "    [:int(len(x_train)*args.rat)] 表示取从索引0到第 int(len(x_train)*args.rat) 个元素（不包括第 int(len(x_train)*args.rat) 个元素）之间的数据。\n",
    "    （按比例划分训练集）并使用torch.tensor()将其转换为张量对象。\n",
    "    类似地，使用x_train[int(len(x_train)*args.rat):]语句获取了 x_train 数据的后 (1-args.rat) 比例部分（按比例划分验证集）\n",
    "    ，并将其转换为张量对象。[int(len(x_train)*args.rat):] 表示取从索引第 int(len(x_train)*args.rat) 个元素到最后一个元素之间的数据。\n",
    "    最后，将训练集和验证集的张量对象分别赋值给 x_train 和 x_dev 变量。这样，你就可以在训练过程中使用这些张量作为输入数据。\n",
    "    \"\"\"\n",
    "    # 这里把存放着edge_90.csv的图节点关系x_mask\n",
    "    # 和存放着边的信息的x_edge_df进行分割，分割成训练集和测试集\n",
    "    # 依然是前len(x_train)*args.rat天的数据作为训练集，其余的作为测试集\n",
    "    x_mask_train,x_mask_dev = torch.tensor(x_mask[:int(len(x_mask)*args.rat)]),torch.tensor(x_mask[int(len(x_mask)*args.rat):])\n",
    "   \n",
    "    x_edge_train, x_edge_dev = torch.tensor(x_edge_df[:int(len(x_edge_df) * args.rat)]),torch.tensor( x_edge_df[int(len(x_edge_df) * args.rat):])\n",
    "\n",
    "\n",
    "\n",
    "    date_emb = 5  # 将每个日期映射到的固定长度向量的维度。\n",
    "     # 这里的x包含了date_id+F35个特征+2个y值的\n",
    "    # train_activate = torch.tensor(y_train[:, -2])\n",
    "    # train_consume = torch.tensor(y_train[:, -1])\n",
    "\n",
    "\n",
    "    # rmse_loss = torch.sqrt(mse_loss)\n",
    "    #model = GAT(date_emb =[len(date_df_dict),date_emb], nfeat=35, nhid=64, dropout=0.3, alpha=0.3, nheads=8).to(args.device)\n",
    "    \"\"\"\n",
    "    nfeat=35   输入特征的维度为 35\n",
    "    nhid=64    隐藏层的维度为 64\n",
    "    dropout=0.3，用于防止过拟合。表示有30%的神经元在前向传播过程中会被随机地丢弃。\n",
    "    \"\"\"\n",
    "    model = BILSTM(date_emb =[len(date_df_dict),date_emb], nfeat=35, nhid=64, dropout=0.3, alpha=0.3, nheads=8).to(args.device)\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),lr=args.lr)\n",
    "    \"\"\"\n",
    "    优化器对象，使用了Adam算法来更新模型的参数\n",
    "    Adam是一种常用的梯度下降法，可以自适应地调整每个参数的学习率，并且能更快的收敛于最优解\n",
    "    我们可以在训练模型的过程中，根据损失函数的反向传播梯度来更新模型的参数。 \n",
    "    \"\"\"\n",
    "    \n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.decline, gamma=0.5, last_epoch=-1)\n",
    "    model.train()\n",
    "    trainset = DataIterator(x_train,x_mask_train,x_edge_train, args)\n",
    "    valset =DataIterator(x_dev,x_mask_dev,x_edge_dev, args)\n",
    "\n",
    " \n",
    "    for indx in range(args.epochs):\n",
    "        train_all_loss = 0.0\n",
    "        for j in trange(trainset.batch_count):\n",
    "            x_date,x_feature,x_mask_data,x_edge_data,x_tags= trainset.get_batch(j)\n",
    "            act_pre, con_pre = model(x_date,x_feature,x_mask_data)\n",
    "            predict = torch.cat((act_pre, con_pre), dim=-1)\n",
    "            loss = criterion(predict, x_tags)\n",
    "            train_all_loss += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('this epoch train loss :{0}'.format(train_all_loss))\n",
    "        # scheduler.step()\n",
    "        eval(model,valset, args)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c020646",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:47<00:00, 47.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4912.0947265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 5004.72802734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:48<00:00, 48.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4879.2412109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4982.17626953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:45<00:00, 45.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4846.3544921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4959.45947265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:48<00:00, 48.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4813.43896484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4936.47314453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:46<00:00, 46.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4779.56787109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4913.06298828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:45<00:00, 45.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4745.19091796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4889.2177734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:42<00:00, 42.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4709.7763671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4864.87890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:50<00:00, 50.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4673.453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4840.09765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:57<00:00, 57.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4634.58984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4814.6611328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:46<00:00, 46.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4594.771484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4788.21142578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:43<00:00, 43.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4552.158203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4760.76318359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:51<00:00, 51.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4507.4423828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4732.322265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:54<00:00, 54.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4460.97607421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4702.74609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:58<00:00, 58.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4411.16796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4671.8525390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:50<00:00, 50.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4359.91796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4639.57080078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:51<00:00, 51.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4304.287109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4605.82080078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:52<00:00, 52.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4245.37158203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4570.51123046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:47<00:00, 47.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4183.9736328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4533.62451171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:51<00:00, 51.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4120.31201171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4495.1806640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:48<00:00, 48.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :4052.06982421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4455.19775390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:01<00:00, 61.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :3980.596923828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4413.6494140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:51<00:00, 51.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :3905.1845703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4370.6220703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:59<00:00, 59.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :3826.927734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4326.1376953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:54<00:00, 54.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :3745.93994140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4280.23095703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:59<00:00, 59.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :3659.262451171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4232.94970703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:56<00:00, 56.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :3571.75341796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4184.34521484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:46<00:00, 46.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :3479.56689453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4134.51318359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:52<00:00, 52.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :3386.04052734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4083.484619140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:51<00:00, 51.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :3286.212158203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 4031.288818359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:58<00:00, 58.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :3184.01318359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3978.014892578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:48<00:00, 48.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :3074.097900390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3923.660888671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:53<00:00, 53.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :2966.194091796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3868.416259765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:47<00:00, 47.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :2856.640869140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3812.15869140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:51<00:00, 51.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :2737.968994140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3755.26318359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:52<00:00, 52.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :2621.047119140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3697.7021484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:54<00:00, 54.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :2505.2548828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3639.5390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:53<00:00, 53.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :2384.0224609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3580.7890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:52<00:00, 52.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :2261.152099609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3521.8623046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:50<00:00, 50.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :2136.224853515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3463.231201171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:45<00:00, 45.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :2014.5802001953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3405.165771484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:55<00:00, 55.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :1889.4776611328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3347.6328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:54<00:00, 54.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :1765.3360595703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3290.361328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:53<00:00, 53.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :1646.8624267578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3233.161865234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:51<00:00, 51.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :1531.2166748046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3176.027099609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:49<00:00, 49.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :1417.680419921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3119.308837890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:48<00:00, 48.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :1307.052001953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3062.701416015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:52<00:00, 52.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :1203.455078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 3006.358642578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:51<00:00, 51.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :1099.368408203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 2951.142822265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:47<00:00, 47.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch train loss :1008.6087646484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this epoch dev loss is 2897.560546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "min_loss = float('inf')\n",
    "# 定义一个类似 argparse 的命名元组\n",
    "from collections import namedtuple\n",
    "Args = namedtuple('Args', ['epochs', 'batch_size', 'device', 'lr', 'rat', 'decline'])\n",
    "\n",
    "# 创建一个 Args 对象并设置各个参数的值\n",
    "args = Args(50, 512, 'cpu', 1e-4, 0.9, 30)\n",
    "\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIterator2(object):\n",
    "    def __init__(self, x_data,x_mask_data,x_edge_data, args):\n",
    "        self.x_data,self.x_mask_data,self.x_edge_data,=x_data,x_mask_data,x_edge_data,\n",
    "        #date跟fearture的分开\n",
    "        self.x_date,self.x_feature=self.x_data[:,:,0],self.x_data[:,:,1:]\n",
    "        # print(self.x_date.shape,self.x_feature.shape,self.x_tags.shape)\n",
    "        self.args = args\n",
    "        self.batch_count = math.ceil(len(x_data)/args.batch_size)\n",
    "\n",
    "    def get_batch(self, index):\n",
    "        x_date = []\n",
    "        x_feature = []\n",
    "        x_mask_data=[]\n",
    "        x_edge_data = []\n",
    "\n",
    "        for i in range(index * self.args.batch_size,\n",
    "                       min((index + 1) * self.args.batch_size, len(self.x_data))):\n",
    "\n",
    "            x_date.append(self.x_date[i])\n",
    "            x_feature.append(self.x_feature[i].float() )\n",
    "\n",
    "            # print(self.x_mask_data[i].shape)\n",
    "            x_mask_data.append(self.x_mask_data[i])\n",
    "            # print(self.x_edge_data[i].shape)\n",
    "            x_edge_data.append(self.x_edge_data[i])\n",
    "\n",
    "        x_date = torch.stack(x_date).to(self.args.device)\n",
    "        x_feature = torch.DoubledTensor(torch.stack(x_feature)).to(self.args.device)\n",
    "        x_mask_data = torch.stack(x_mask_data).to(self.args.device)\n",
    "        x_edge_data = torch.stack(x_edge_data).to(self.args.device)\n",
    "\n",
    "\n",
    "        return  x_date,x_feature,x_mask_data,x_edge_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73d778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1140, 1])\n",
      "torch.Size([4, 1140, 1])\n",
      "torch.Size([4560])\n",
      "torch.Size([4560])\n"
     ]
    }
   ],
   "source": [
    "node_csv = pd.read_csv('node_test_4_A.csv')\n",
    "edge_csv = pd.read_csv('edge_test_4_A.csv')\n",
    "geohash_id = node_csv['geohash_id']\n",
    "date_id = node_csv['date_id']\n",
    "geohasd_df_dict2, date_df_dict2, x_data2, x_mask2, x_edge_df2 = get_test_data('node_test_4_A.csv','edge_test_4_A.csv')#得到测试集数据\n",
    "x_data2 = torch.tensor(x_data2[:])\n",
    "x_mask2 = torch.tensor(x_mask2[:])\n",
    "x_edge_df2 = torch.tensor(x_edge_df2[:])\n",
    "outset=DataIterator2 (x_data2,x_mask2,x_edge_df2, args)\n",
    "predict = torch.Tensor()\n",
    "model1 = torch.load('best_model.pth')\n",
    "with torch.no_grad():\n",
    "    act_pre, con_pre = model1(outset.x_date,outset.x_feature,outset.x_mask_data)\n",
    "    print(act_pre.shape)\n",
    "    print(con_pre.shape)\n",
    "    act_pre=act_pre.reshape(1,4560)\n",
    "    con_pre=con_pre.reshape(1,4560)\n",
    "    predict = torch.cat((act_pre, con_pre), dim= 0)\n",
    "        \n",
    "\n",
    "consumption_level = predict[1,:].to('cpu')\n",
    "activity_level = predict[0, :].to('cpu')\n",
    "print(consumption_level.shape)\n",
    "print(activity_level.shape)\n",
    "\n",
    "output = {\n",
    "    'geohash_id': geohash_id.tolist(),\n",
    "    'consumption_level': consumption_level.tolist(),\n",
    "    'activity_level': activity_level.tolist(),\n",
    "    'date_id': date_id.tolist()\n",
    "}\n",
    "df = pd.DataFrame.from_dict(output)\n",
    "df.to_csv('output2.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e715d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d862bbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc18ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f20c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a48bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec64419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd2bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629cc182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea5436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b68b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048b389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6ba3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2861b45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6a48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01f97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20186a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5956969c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e3e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaea038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfffd4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81579b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model = GAT(date_emb =[124,6], nfeat=35, nhid=64, dropout=0.3, alpha=0.3, nheads=8).to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd979c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'my_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.load('my_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a4056",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = model.load_state_dict(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294dffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
