{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d54c592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:15.352250700Z",
     "start_time": "2023-11-18T01:45:15.340146700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 读取CSV文件\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# tqdm函数用于为循环或迭代器创建进度条。\n",
    "# 它可以用于显示需要很长时间才能完成的任务的进度，例如数据处理或模型训练。\n",
    "# trange函数类似于Python中的range函数，但它还创建了有指定迭代次数的进度条。\n",
    "# 这允许您实时查看循环的进度，因此可以更好地跟踪任务执行情况。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51654725",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:17.028602500Z",
     "start_time": "2023-11-18T01:45:17.012600600Z"
    }
   },
   "outputs": [],
   "source": [
    "# import my_model   # my_model是自己写的一个文件\n",
    "# import data # data是一个自己写的文件\n",
    "import torch.nn as nn\n",
    "\"\"\"\n",
    "这里导入了pytorch深度学习的nn模块，这个模块提供了神经网络层、损失函数\n",
    "和优化器等工具的类，通过这个模块可以方便地构建和训练神经网络模型。\n",
    "\"\"\"\n",
    "min_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea19016",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:21.802666500Z",
     "start_time": "2023-11-18T01:45:21.782666900Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()  # 使用均方误差损失函数计算MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e54d138b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:22.984016800Z",
     "start_time": "2023-11-18T01:45:22.969024400Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import  pickle\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d387ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:26.292549900Z",
     "start_time": "2023-11-18T01:45:26.270504800Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataIterator(object):\n",
    "    def __init__(self, x_data,x_mask_data,x_edge_data, args):\n",
    "        self.x_data,self.x_mask_data,self.x_edge_data,=x_data,x_mask_data,x_edge_data,\n",
    "        #date跟fearture的分开\n",
    "        # 虽然x_date是一个二维数组，但是二维数组中的每个元素都是一个列表，每个列表的内容是一个日期id和三十五个F特征还有要预测的两个目标值\n",
    "        # x_data[:,:,0]取出来的是日期\n",
    "        # x_data[:,:,1:-2]取出来的是三十五个特征\n",
    "        # x_data[:,:,-2:]取出来的是两个预测值也就是\n",
    "        self.x_date,self.x_feature,self.x_tags=self.x_data[:,:,0],self.x_data[:,:,1:-2],x_data[:,:,-2:]\n",
    "        # print(self.x_date.shape,self.x_feature.shape,self.x_tags.shape)\n",
    "        self.args = args\n",
    "        #通过数据总数除掉每个批次的数据数目args.batch_size来算出一共多少个批次\n",
    "        self.batch_count = math.ceil(len(x_data)/args.batch_size)\n",
    "\n",
    "        #get_batch函数是用来获取某个训练批次的数据的，index代表批次号\n",
    "    def get_batch(self, index):\n",
    "        x_date = []\n",
    "        x_feature = []\n",
    "        x_mask_data=[]\n",
    "        x_edge_data = []\n",
    "        x_tags = []\n",
    "\n",
    "\n",
    "        for i in range(index * self.args.batch_size,\n",
    "                       min((index + 1) * self.args.batch_size, len(self.x_data))):\n",
    "\n",
    "            x_date.append(self.x_date[i])\n",
    "            x_feature.append(self.x_feature[i].float() )\n",
    "\n",
    "            # print(self.x_mask_data[i].shape)\n",
    "            x_mask_data.append(self.x_mask_data[i])\n",
    "            # print(self.x_edge_data[i].shape)\n",
    "            x_edge_data.append(self.x_edge_data[i])\n",
    "            x_tags.append(self.x_tags[i].float() )\n",
    "\n",
    "        x_date = torch.stack(x_date).to(self.args.device)\n",
    "        x_feature = torch.FloatTensor(torch.stack(x_feature)).to(self.args.device)\n",
    "        x_mask_data = torch.stack(x_mask_data).to(self.args.device)\n",
    "        x_edge_data = torch.stack(x_edge_data).to(self.args.device)\n",
    "        x_tags = torch.stack(x_tags).to(self.args.device)\n",
    "\n",
    "\n",
    "        return  x_date,x_feature,x_mask_data,x_edge_data,x_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6286b3f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:26.959169800Z",
     "start_time": "2023-11-18T01:45:26.947167100Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07fe3424",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:28.043223400Z",
     "start_time": "2023-11-18T01:45:28.020219400Z"
    }
   },
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout, bias=False):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)  # xavier初始化，就是论文里的glorot初始化\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "            nn.init.zeros_(self.bias)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, inputs, adj):\n",
    "        # inputs: (N, n_channels), adj: sparse_matrix (N, N)\n",
    "        support = torch.mm(self.dropout(inputs), self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f60ec6db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:29.006845700Z",
     "start_time": "2023-11-18T01:45:28.983840500Z"
    }
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim, dropout, n_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(n_features, hidden_dim, dropout)\n",
    "        self.gc2 = GraphConvolution(hidden_dim, n_classes, dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs, adj):\n",
    "        x = inputs\n",
    "        x = self.relu(self.gc1(x, adj))\n",
    "        x = self.gc2(x, adj)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3e6f5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:29.882926900Z",
     "start_time": "2023-11-18T01:45:29.878915800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08dca4b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:30.830277300Z",
     "start_time": "2023-11-18T01:45:30.805271300Z"
    }
   },
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(in_features, out_features))\n",
    "        self.a = nn.Parameter(torch.zeros(2 * out_features, 1))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        '''\n",
    "        h: (N, in_features)\n",
    "        adj: sparse matrix with shape (N, N)\n",
    "        p\n",
    "        '''\n",
    "        adj=torch.squeeze(adj,-1)\n",
    "        # print(h.dtype)\n",
    "        # print(h.shape)\n",
    "        h = h.type_as(self.W)\n",
    "        Wh = torch.matmul(h, self.W)  # (N, out_features)\n",
    "        #print(h)\n",
    "        #print(self.W)\n",
    "\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])  # (N, 1)\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])  # (N, 1)\n",
    "        # print(Wh1.shape)\n",
    "        # print(Wh2.shape)\n",
    "\n",
    "        # Wh1 + Wh2.T 是N*N矩阵，第i行第j列是Wh1[i]+Wh2[j]\n",
    "        # 那么Wh1 + Wh2.T的第i行第j列刚好就是文中的a^T*[Whi||Whj]\n",
    "        # 代表着节点i对节点j的attention\n",
    "        # print(torch.transpose(Wh2,2,1).shape)\n",
    "        e = self.leakyrelu(Wh1 +torch.transpose(Wh2,2,1))  # (N, N)\n",
    "        padding = (-2 ** 31) * torch.ones_like(e)  # (N, N)\n",
    "        # print(adj.shape)\n",
    "        # print(padding.shape)\n",
    "        attention = torch.where(adj > 0, e, padding)  # (N, N)\n",
    "        attention = F.softmax(attention, dim=1)  # (N, N)\n",
    "        # attention矩阵第i行第j列代表node_i对node_j的注意力\n",
    "        # 对注意力权重也做dropout（如果经过mask之后，attention矩阵也许是高度稀疏的，这样做还有必要吗？）\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "\n",
    "        h_prime = torch.matmul(attention, Wh)  # (N, out_features)\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44813ab9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:31.549508300Z",
     "start_time": "2023-11-18T01:45:31.520501400Z"
    }
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self,date_emb, nfeat, nhid, dropout, alpha, nheads):\n",
    "        super(GAT, self).__init__()\n",
    "        date_index_number,date_dim = date_emb[0], date_emb[1]\n",
    "        self.dropout = dropout\n",
    "        self.MH = nn.ModuleList([\n",
    "            GraphAttentionLayer(nfeat, nhid, dropout, alpha, concat=True)\n",
    "            for _ in range(nheads)\n",
    "        ])\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nhid, dropout, alpha, concat=False)\n",
    "        self.date_embdding = nn.Embedding(date_index_number,date_dim)\n",
    "        self.active_index = nn.Linear(nhid,1)\n",
    "        self.consume_index = nn.Linear(nhid,1)\n",
    "    def forward(self,x_date,x_feature,x_mask_data):\n",
    "\n",
    "\n",
    "        x = x_feature\n",
    "        # x = F.dropout(x_feature, self.dropout, training=self.training)  # (N, nfeat)\n",
    "        x = torch.cat([head(x, x_mask_data) for head in self.MH], dim=-1)  # (N, nheads*nhid)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)  # (N, nfeat)\n",
    "\n",
    "\n",
    "        # x = F.dropout(x, self.dropout, training=self.training)  # (N, nheads*nhid)\n",
    "        x = self.out_att(x, x_mask_data)\n",
    "        # print(x.shape,x.dtype)\n",
    "        act_pre= self.active_index(x)\n",
    "        con_pre = self.consume_index(x)\n",
    "        return  act_pre,con_pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e39e4ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:32.249962400Z",
     "start_time": "2023-11-18T01:45:32.209951700Z"
    }
   },
   "outputs": [],
   "source": [
    "class BILSTM(nn.Module):\n",
    "    def __init__(self,date_emb, nfeat, nhid, dropout, alpha, nheads):\n",
    "        super(BILSTM, self).__init__()\n",
    "        date_index_number,date_dim = date_emb[0], date_emb[1]\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(nfeat,\n",
    "                nhid,\n",
    "                num_layers=2,\n",
    "                bias=True,\n",
    "                batch_first=False,\n",
    "                dropout=0,\n",
    "                bidirectional=True)\n",
    "\n",
    "        self.active_index = nn.Linear(2*nhid, 1)\n",
    "        self.consume_index = nn.Linear(2*nhid, 1)\n",
    "    def forward(self,x_date,x_feature,x_mask_data):\n",
    "        x_feature = x_feature.double()\n",
    "        lstm_out, (hidden, cell) = self.lstm(x_feature)\n",
    "        x = lstm_out\n",
    "        # print(x.shape)\n",
    "\n",
    "\n",
    "        x = F.dropout(x, self.dropout, training=self.training)  # (N, nheads*nhid)\n",
    "        act_pre= self.active_index(x)\n",
    "        con_pre = self.consume_index(x)\n",
    "        # print(act_pre.shape,con_pre.shape)\n",
    "        return  act_pre,con_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1401894c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:37.578979Z",
     "start_time": "2023-11-18T01:45:37.567977200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd2df9cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:38.478856400Z",
     "start_time": "2023-11-18T01:45:38.431829800Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_data(file_path,edge_pth):\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    #读取图文件，并且把它存储为一个名字为df的对象，并且指定文件字符编码为UFT-8编码\n",
    "    edge_df = pd.read_csv(edge_pth, encoding='utf-8')\n",
    "    df.head()\n",
    "\n",
    "    # %%\n",
    "    # 先定义两个空字典变量，分别用来存储地点和日期和某个序号的映射\n",
    "    geohasd_df_dict = {}\n",
    "    date_df_dict = {}\n",
    "    #最开始字典为空，先声明两个初始值为0的变量\n",
    "    number_hash = 0\n",
    "    number_date = 0\n",
    "    \n",
    "    # 使用循环\n",
    "    for i in df[\"geohash_id\"]:\n",
    "\n",
    "        if i not in geohasd_df_dict.keys():\n",
    "            geohasd_df_dict[i] = number_hash\n",
    "            number_hash += 1 #字典已经存入了一个地点，那么number_hash就应该加1\n",
    "            \n",
    "    for i in df[\"date_id\"]:\n",
    "        if i not in date_df_dict.keys():\n",
    "            date_df_dict[i] = number_date\n",
    "            number_date += 1\n",
    "\n",
    "    # 这里创建了一个列表，行数就是日期的数目，列数是地点的数目\n",
    "    # 子列表数目就是\n",
    "    new_data = np.zeros((len(date_df_dict),len(geohasd_df_dict),38))\n",
    "    # [len(geohasd_df_dict) * [0]] * len(date_df_dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "    # iterrows()方法会返回一个元组，其中包含当前行的索引和数据\n",
    "    # index变量将保存当前行的索引，row变量将保存当前行的数据\n",
    "        # print(index)\n",
    "        hash_index, date_index = geohasd_df_dict[row[\"geohash_id\"]], date_df_dict[row[\"date_id\"]]\n",
    "        #将时间index加到里面\n",
    "        \"\"\"\n",
    "        这里new_data[date_index][hash_index]将被赋值为一个列表。\n",
    "        这个列表的第一个元素是date_index，后面是row.iloc[2:]的值。\n",
    "        row.iloc[2:]表示从row中的第三个元素开始到最后一个元素的切片。\n",
    "        包含一行中从第三列开始的所有列的值。\n",
    "        \"\"\"\n",
    "        new_data[date_index][hash_index] = [date_index]+list(row.iloc[2:])\n",
    "    new_data = np.array(new_data) \n",
    "    # 这里new_data转换为numpy数组，这个函数会创建一个具有相同维度和元素的新数组\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # x_train,y_train = new_data[:, :-2], new_data[:, -2:]\n",
    "    # print(len(geohasd_df_dict))\n",
    "    # exit()\n",
    "    # print(x_train.shape)\n",
    "    # print(y_train.shape)\n",
    "    #这里构建邻接矩阵其中mask表示1为有边，0无边， value_mask表示有值\n",
    "    #并且这里我考虑mask是一个无向图，如果有向删除x_mask[date_index][point2_index][point1_index],value_mask同理\n",
    "    \n",
    "    #下面两个是元素值均为0的数组\n",
    "    x_mask =  np.zeros((len(date_df_dict),len(geohasd_df_dict),len(geohasd_df_dict),1), dtype = float)\n",
    "    x_edge_df =np.zeros((len(date_df_dict),len(geohasd_df_dict),len(geohasd_df_dict),2), dtype = float)\n",
    "\n",
    "    for index, row in edge_df.iterrows():\n",
    "        # print(index)\n",
    "        # 地点编号在字典中找不到，就说明这个数据是错误的，出现了错误的地点，那就进入下一层循环\n",
    "        if row[\"geohash6_point1\"] not in geohasd_df_dict.keys() or row[\"geohash6_point2\"] not in geohasd_df_dict.keys():\n",
    "            continue\n",
    "        point1_index,point2_index,F_1,F_2,date_index= geohasd_df_dict[row[\"geohash6_point1\"]],geohasd_df_dict[row[\"geohash6_point2\"]]\\\n",
    "            ,row[\"F_1\"],row[\"F_2\"],date_df_dict[row[\"date_id\"]]\n",
    "        x_mask[date_index][point1_index][point2_index] = 1\n",
    "        x_mask[date_index][point2_index][point1_index] = 1\n",
    "        # 这里把mask数组对应位置赋值为1，说明两个地点之间有边\n",
    "        x_edge_df[date_index][point1_index][point2_index] =  [F_1,F_2]\n",
    "        x_edge_df[date_index][point2_index][point1_index] = [F_1, F_2]\n",
    "        # 把边上面的两个特征存入数组x_edge_df中\n",
    "    # print(data)\n",
    "\n",
    "    return     geohasd_df_dict, date_df_dict, new_data,x_mask, x_edge_df\n",
    "#get_train_data函数运行完之后，把地点、时间字典等各种数据做一个返回\n",
    "#newdata存的是某个日期某个地点的具体特征\n",
    "#x_mask 存的是某个日期两个地点之间是否有边\n",
    "#x_edge_df存的是边上的两个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbcecfee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:39.621402600Z",
     "start_time": "2023-11-18T01:45:39.575378200Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_test_data(file_path,edge_pth):\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    #读取图文件，并且把它存储为一个名字为df的对象，并且指定文件字符编码为UFT-8编码\n",
    "    edge_df = pd.read_csv(edge_pth, encoding='utf-8')\n",
    "    df.head()\n",
    "\n",
    "    # %%\n",
    "    # 先定义两个空字典变量，分别用来存储地点和日期和某个序号的映射\n",
    "    geohasd_df_dict = {}\n",
    "    date_df_dict = {}\n",
    "    #最开始字典为空，先声明两个初始值为0的变量\n",
    "    number_hash = 0\n",
    "    number_date = 0\n",
    "    \n",
    "    # 使用循环\n",
    "    for i in df[\"geohash_id\"]:\n",
    "\n",
    "        if i not in geohasd_df_dict.keys():\n",
    "            geohasd_df_dict[i] = number_hash\n",
    "            number_hash += 1 #字典已经存入了一个地点，那么number_hash就应该加1\n",
    "            \n",
    "    for i in df[\"date_id\"]:\n",
    "        if i not in date_df_dict.keys():\n",
    "            date_df_dict[i] = number_date\n",
    "            number_date += 1\n",
    "\n",
    "    # 这里创建了一个列表，行数就是日期的数目，列数是地点的数目\n",
    "    # 子列表数目就是\n",
    "    new_data = np.zeros((len(date_df_dict),len(geohasd_df_dict),36))\n",
    "    # [len(geohasd_df_dict) * [0]] * len(date_df_dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "    # iterrows()方法会返回一个元组，其中包含当前行的索引和数据\n",
    "    # index变量将保存当前行的索引，row变量将保存当前行的数据\n",
    "        # print(index)\n",
    "        hash_index, date_index = geohasd_df_dict[row[\"geohash_id\"]], date_df_dict[row[\"date_id\"]]\n",
    "        #将时间index加到里面\n",
    "        \"\"\"\n",
    "        这里new_data[date_index][hash_index]将被赋值为一个列表。\n",
    "        这个列表的第一个元素是date_index，后面是row.iloc[2:]的值。\n",
    "        row.iloc[2:]表示从row中的第三个元素开始到最后一个元素的切片。\n",
    "        包含一行中从第三列开始的所有列的值。\n",
    "        \"\"\"\n",
    "        new_data[date_index][hash_index] = [date_index]+list(row.iloc[2:])\n",
    "    new_data = np.array(new_data) \n",
    "    # 这里new_data转换为numpy数组，这个函数会创建一个具有相同维度和元素的新数组\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # x_train,y_train = new_data[:, :-2], new_data[:, -2:]\n",
    "    # print(len(geohasd_df_dict))\n",
    "    # exit()\n",
    "    # print(x_train.shape)\n",
    "    # print(y_train.shape)\n",
    "    #这里构建邻接矩阵其中mask表示1为有边，0无边， value_mask表示有值\n",
    "    #并且这里我考虑mask是一个无向图，如果有向删除x_mask[date_index][point2_index][point1_index],value_mask同理\n",
    "    \n",
    "    #下面两个是元素值均为0的数组\n",
    "    x_mask =  np.zeros((len(date_df_dict),len(geohasd_df_dict),len(geohasd_df_dict),1), dtype = float)\n",
    "    x_edge_df =np.zeros((len(date_df_dict),len(geohasd_df_dict),len(geohasd_df_dict),2), dtype = float)\n",
    "\n",
    "    for index, row in edge_df.iterrows():\n",
    "        # print(index)\n",
    "        # 地点编号在字典中找不到，就说明这个数据是错误的，出现了错误的地点，那就进入下一层循环\n",
    "        if row[\"geohash6_point1\"] not in geohasd_df_dict.keys() or row[\"geohash6_point2\"] not in geohasd_df_dict.keys():\n",
    "            continue\n",
    "        point1_index,point2_index,F_1,F_2,date_index= geohasd_df_dict[row[\"geohash6_point1\"]],geohasd_df_dict[row[\"geohash6_point2\"]]\\\n",
    "            ,row[\"F_1\"],row[\"F_2\"],date_df_dict[row[\"date_id\"]]\n",
    "        x_mask[date_index][point1_index][point2_index] = 1\n",
    "        x_mask[date_index][point2_index][point1_index] = 1\n",
    "        # 这里把mask数组对应位置赋值为1，说明两个地点之间有边\n",
    "        x_edge_df[date_index][point1_index][point2_index] =  [F_1,F_2]\n",
    "        x_edge_df[date_index][point2_index][point1_index] = [F_1, F_2]\n",
    "        # 把边上面的两个特征存入数组x_edge_df中\n",
    "    # print(data)\n",
    "\n",
    "    return     geohasd_df_dict, date_df_dict, new_data,x_mask, x_edge_df\n",
    "#get_train_data函数运行完之后，把地点、时间字典等各种数据做一个返回\n",
    "#newdata存的是某个日期某个地点的具体特征\n",
    "#x_mask 存的是某个日期两个地点之间是否有边\n",
    "#x_edge_df存的是边上的两个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbe3e596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:45.202851800Z",
     "start_time": "2023-11-18T01:45:45.181846400Z"
    }
   },
   "outputs": [],
   "source": [
    "# 这是一个评估函数，评估模型在给定数据集上的性能\n",
    "# model是要评估的模型\n",
    "# dataset是要用于评估模型的数据的数据集对象\n",
    "# args是包含其他辅助参数的对象\n",
    "def eval(model, dataset, args):\n",
    "    global min_loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        dev_loss = 0.0\n",
    "        for j in trange(dataset.batch_count):\n",
    "            x_date, x_feature, x_mask_data, x_edge_data, x_tags = dataset.get_batch(j)\n",
    "            act_pre, con_pre = model(x_date, x_feature, x_mask_data)\n",
    "            predict = torch.cat((act_pre, con_pre), dim=-1)\n",
    "            loss = criterion(predict, x_tags)\n",
    "            dev_loss+= loss\n",
    "        print(\"this epoch dev loss is {}\".format(dev_loss))\n",
    "        if  dev_loss < min_loss:\n",
    "            min_loss = dev_loss\n",
    "            # best_model_params = model.state_dict()\n",
    "            torch.save(model, 'best_model_{}.pth'.format(dev_loss))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f82dfdb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:45:47.189312300Z",
     "start_time": "2023-11-18T01:45:47.172308300Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(args):\n",
    "\n",
    "    #args是一个参数，它有两个特征args.rat还有args.batch_size\n",
    "    #args.rat代表要把文件里面的取出来多少百分比作为训练集\n",
    "    #args.batch_size代表一个批次要多少数据\n",
    "    \n",
    "    geohasd_df_dict, date_df_dict, x_train, x_mask, x_edge_df = get_train_data('./train_90.csv',\n",
    "                                                                                        \"./edge_90.csv\")\n",
    "    # 这里的x_train存的是各个地点及其身上的各个特征\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #分割各种训练集测试集\n",
    "    # 这里把存放着train_90.csv的矩阵x_train进行分割，分割成训练集和测试集\n",
    "    x_train,x_dev = torch.tensor(x_train[:int(len(x_train)*args.rat)]),torch.tensor(x_train[int(len(x_train)*args.rat):])\n",
    "    # x_train训练集取了前len(x_train)*args.rat天的数据，x_dev测试集取了后len(x_train)*args.rat天的数据。\n",
    "    \"\"\"\n",
    "    x_train[:int(len(x_train)*args.rat)]语句获取了 x_train 数据的前 args.rat 比例部分\n",
    "    [:int(len(x_train)*args.rat)] 表示取从索引0到第 int(len(x_train)*args.rat) 个元素（不包括第 int(len(x_train)*args.rat) 个元素）之间的数据。\n",
    "    （按比例划分训练集）并使用torch.tensor()将其转换为张量对象。\n",
    "    类似地，使用x_train[int(len(x_train)*args.rat):]语句获取了 x_train 数据的后 (1-args.rat) 比例部分（按比例划分验证集）\n",
    "    ，并将其转换为张量对象。[int(len(x_train)*args.rat):] 表示取从索引第 int(len(x_train)*args.rat) 个元素到最后一个元素之间的数据。\n",
    "    最后，将训练集和验证集的张量对象分别赋值给 x_train 和 x_dev 变量。这样，你就可以在训练过程中使用这些张量作为输入数据。\n",
    "    \"\"\"\n",
    "    # 这里把存放着edge_90.csv的图节点关系x_mask\n",
    "    # 和存放着边的信息的x_edge_df进行分割，分割成训练集和测试集\n",
    "    # 依然是前len(x_train)*args.rat天的数据作为训练集，其余的作为测试集\n",
    "    x_mask_train,x_mask_dev = torch.tensor(x_mask[:int(len(x_mask)*args.rat)]),torch.tensor(x_mask[int(len(x_mask)*args.rat):])\n",
    "   \n",
    "    x_edge_train, x_edge_dev = torch.tensor(x_edge_df[:int(len(x_edge_df) * args.rat)]),torch.tensor( x_edge_df[int(len(x_edge_df) * args.rat):])\n",
    "\n",
    "\n",
    "\n",
    "    date_emb = 5  # 将每个日期映射到的固定长度向量的维度。\n",
    "     # 这里的x包含了date_id+F35个特征+2个y值的\n",
    "    # train_activate = torch.tensor(y_train[:, -2])\n",
    "    # train_consume = torch.tensor(y_train[:, -1])\n",
    "\n",
    "\n",
    "    # rmse_loss = torch.sqrt(mse_loss)\n",
    "    #model = GAT(date_emb =[len(date_df_dict),date_emb], nfeat=35, nhid=64, dropout=0.3, alpha=0.3, nheads=8).to(args.device)\n",
    "    \"\"\"\n",
    "    nfeat=35   输入特征的维度为 35\n",
    "    nhid=64    隐藏层的维度为 64\n",
    "    dropout=0.3，用于防止过拟合。表示有30%的神经元在前向传播过程中会被随机地丢弃。\n",
    "    \"\"\"\n",
    "    model = BILSTM(date_emb =[len(date_df_dict),date_emb], nfeat=35, nhid=64, dropout=0.3, alpha=0.3, nheads=8).to(args.device)\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),lr=args.lr)\n",
    "    \"\"\"\n",
    "    优化器对象，使用了Adam算法来更新模型的参数\n",
    "    Adam是一种常用的梯度下降法，可以自适应地调整每个参数的学习率，并且能更快的收敛于最优解\n",
    "    我们可以在训练模型的过程中，根据损失函数的反向传播梯度来更新模型的参数。 \n",
    "    \"\"\"\n",
    "    \n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.decline, gamma=0.5, last_epoch=-1)\n",
    "    model.train()\n",
    "    trainset = DataIterator(x_train,x_mask_train,x_edge_train, args)\n",
    "    valset =DataIterator(x_dev,x_mask_dev,x_edge_dev, args)\n",
    "\n",
    " \n",
    "    for indx in range(args.epochs):\n",
    "        train_all_loss = 0.0\n",
    "        for j in trange(trainset.batch_count):\n",
    "            x_date,x_feature,x_mask_data,x_edge_data,x_tags= trainset.get_batch(j)\n",
    "            act_pre, con_pre = model(x_date,x_feature,x_mask_data)\n",
    "            predict = torch.cat((act_pre, con_pre), dim=-1)\n",
    "            loss = criterion(predict, x_tags)\n",
    "            train_all_loss += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('this epoch train loss :{0}'.format(train_all_loss))\n",
    "        # scheduler.step()\n",
    "        eval(model,valset, args)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c020646",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T01:46:53.122299300Z",
     "start_time": "2023-11-18T01:45:51.632597100Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\astudy\\computer\\bdci\\time\\new_python(1).ipynb Cell 18\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# 创建一个 Args 对象并设置各个参数的值\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m args \u001b[39m=\u001b[39m Args(\u001b[39m50\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1e-4\u001b[39m, \u001b[39m0.9\u001b[39m, \u001b[39m30\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train(args)\n",
      "\u001b[1;32md:\\astudy\\computer\\bdci\\time\\new_python(1).ipynb Cell 18\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(args):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m#args是一个参数，它有两个特征args.rat还有args.batch_size\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m#args.rat代表要把文件里面的取出来多少百分比作为训练集\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m#args.batch_size代表一个批次要多少数据\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     geohasd_df_dict, date_df_dict, x_train, x_mask, x_edge_df \u001b[39m=\u001b[39m get_train_data(\u001b[39m'\u001b[39m\u001b[39m./train_90.csv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                                                                         \u001b[39m\"\u001b[39m\u001b[39m./edge_90.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# 这里的x_train存的是各个地点及其身上的各个特征\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m#分割各种训练集测试集\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# 这里把存放着train_90.csv的矩阵x_train进行分割，分割成训练集和测试集\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     x_train,x_dev \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(x_train[:\u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(x_train)\u001b[39m*\u001b[39margs\u001b[39m.\u001b[39mrat)]),torch\u001b[39m.\u001b[39mtensor(x_train[\u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(x_train)\u001b[39m*\u001b[39margs\u001b[39m.\u001b[39mrat):])\n",
      "\u001b[1;32md:\\astudy\\computer\\bdci\\time\\new_python(1).ipynb Cell 18\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m x_mask \u001b[39m=\u001b[39m  np\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(date_df_dict),\u001b[39mlen\u001b[39m(geohasd_df_dict),\u001b[39mlen\u001b[39m(geohasd_df_dict),\u001b[39m1\u001b[39m), dtype \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m x_edge_df \u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(date_df_dict),\u001b[39mlen\u001b[39m(geohasd_df_dict),\u001b[39mlen\u001b[39m(geohasd_df_dict),\u001b[39m2\u001b[39m), dtype \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m edge_df\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     \u001b[39m# print(index)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39m# 地点编号在字典中找不到，就说明这个数据是错误的，出现了错误的地点，那就进入下一层循环\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39mif\u001b[39;00m row[\u001b[39m\"\u001b[39m\u001b[39mgeohash6_point1\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m geohasd_df_dict\u001b[39m.\u001b[39mkeys() \u001b[39mor\u001b[39;00m row[\u001b[39m\"\u001b[39m\u001b[39mgeohash6_point2\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m geohasd_df_dict\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/astudy/computer/bdci/time/new_python%281%29.ipynb#X23sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\pandas\\core\\frame.py:1400\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1398\u001b[0m using_cow \u001b[39m=\u001b[39m using_copy_on_write()\n\u001b[0;32m   1399\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues):\n\u001b[1;32m-> 1400\u001b[0m     s \u001b[39m=\u001b[39m klass(v, index\u001b[39m=\u001b[39mcolumns, name\u001b[39m=\u001b[39mk)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m)\n\u001b[0;32m   1401\u001b[0m     \u001b[39mif\u001b[39;00m using_cow \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mis_single_block:\n\u001b[0;32m   1402\u001b[0m         s\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39madd_references(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\pandas\\core\\series.py:509\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    507\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    508\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 509\u001b[0m     data \u001b[39m=\u001b[39m sanitize_array(data, index, dtype, copy)\n\u001b[0;32m    511\u001b[0m     manager \u001b[39m=\u001b[39m get_option(\u001b[39m\"\u001b[39m\u001b[39mmode.data_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    512\u001b[0m     \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mblock\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\pandas\\core\\construction.py:569\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    567\u001b[0m subarr \u001b[39m=\u001b[39m data\n\u001b[0;32m    568\u001b[0m \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[1;32m--> 569\u001b[0m     subarr \u001b[39m=\u001b[39m maybe_infer_to_datetimelike(data)\n\u001b[0;32m    571\u001b[0m \u001b[39mif\u001b[39;00m subarr \u001b[39mis\u001b[39;00m data \u001b[39mand\u001b[39;00m copy:\n\u001b[0;32m    572\u001b[0m     subarr \u001b[39m=\u001b[39m subarr\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1204\u001b[0m, in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m   1199\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[0;32m   1201\u001b[0m \u001b[39m# error: Incompatible return value type (got \"Union[ExtensionArray,\u001b[39;00m\n\u001b[0;32m   1202\u001b[0m \u001b[39m# ndarray[Any, Any]]\", expected \"Union[ndarray[Any, Any], DatetimeArray,\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m \u001b[39m# TimedeltaArray, PeriodArray, IntervalArray]\")\u001b[39;00m\n\u001b[1;32m-> 1204\u001b[0m \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmaybe_convert_objects(  \u001b[39m# type: ignore[return-value]\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m     value,\n\u001b[0;32m   1206\u001b[0m     \u001b[39m# Here we do not convert numeric dtypes, as if we wanted that,\u001b[39;00m\n\u001b[0;32m   1207\u001b[0m     \u001b[39m#  numpy would have done it for us.\u001b[39;00m\n\u001b[0;32m   1208\u001b[0m     convert_numeric\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1209\u001b[0m     convert_period\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   1210\u001b[0m     convert_interval\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   1211\u001b[0m     convert_timedelta\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   1212\u001b[0m     convert_datetime\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   1213\u001b[0m     dtype_if_all_nat\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mM8[ns]\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1214\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2445\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\numpy\\core\\numeric.py:327\u001b[0m, in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order, like)\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m _full_with_like(\n\u001b[0;32m    324\u001b[0m             like, shape, fill_value, dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39morder)\n\u001b[0;32m    326\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m     fill_value \u001b[39m=\u001b[39m asarray(fill_value)\n\u001b[0;32m    328\u001b[0m     dtype \u001b[39m=\u001b[39m fill_value\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    329\u001b[0m a \u001b[39m=\u001b[39m empty(shape, dtype, order)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# min_loss = float('inf')\n",
    "# 定义一个类似 argparse 的命名元组\n",
    "from collections import namedtuple\n",
    "Args = namedtuple('Args', ['epochs', 'batch_size', 'device', 'lr', 'rat', 'decline'])\n",
    "\n",
    "# 创建一个 Args 对象并设置各个参数的值\n",
    "args = Args(50, 512, 'cpu', 1e-4, 0.9, 30)\n",
    "\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2988ffcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T13:12:17.483362300Z",
     "start_time": "2023-11-16T13:12:17.475359600Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataIterator2(object):\n",
    "    def __init__(self, x_data,x_mask_data,x_edge_data, args):\n",
    "        self.x_data,self.x_mask_data,self.x_edge_data,=x_data,x_mask_data,x_edge_data,\n",
    "        #date跟fearture的分开\n",
    "        self.x_date,self.x_feature=self.x_data[:,:,0],self.x_data[:,:,1:]\n",
    "        # print(self.x_date.shape,self.x_feature.shape,self.x_tags.shape)\n",
    "        self.args = args\n",
    "        self.batch_count = math.ceil(len(x_data)/args.batch_size)\n",
    "\n",
    "    def get_batch(self, index):\n",
    "        x_date = []\n",
    "        x_feature = []\n",
    "        x_mask_data=[]\n",
    "        x_edge_data = []\n",
    "\n",
    "        for i in range(index * self.args.batch_size,\n",
    "                       min((index + 1) * self.args.batch_size, len(self.x_data))):\n",
    "\n",
    "            x_date.append(self.x_date[i])\n",
    "            x_feature.append(self.x_feature[i].float() )\n",
    "\n",
    "            # print(self.x_mask_data[i].shape)\n",
    "            x_mask_data.append(self.x_mask_data[i])\n",
    "            # print(self.x_edge_data[i].shape)\n",
    "            x_edge_data.append(self.x_edge_data[i])\n",
    "\n",
    "        x_date = torch.stack(x_date).to(self.args.device)\n",
    "        x_feature = torch.DoubledTensor(torch.stack(x_feature)).to(self.args.device)\n",
    "        x_mask_data = torch.stack(x_mask_data).to(self.args.device)\n",
    "        x_edge_data = torch.stack(x_edge_data).to(self.args.device)\n",
    "\n",
    "\n",
    "        return  x_date,x_feature,x_mask_data,x_edge_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f73d778",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T13:14:17.473194500Z",
     "start_time": "2023-11-16T13:14:12.453751900Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m model1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_3861.89404296875.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 13\u001b[0m     act_pre, con_pre \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_mask_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(act_pre\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(con_pre\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mD:\\CSSoftware\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[34], line 17\u001b[0m, in \u001b[0;36mBILSTM.forward\u001b[1;34m(self, x_date, x_feature, x_mask_data)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x_date,x_feature,x_mask_data):\n\u001b[1;32m---> 17\u001b[0m     lstm_out, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_feature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m lstm_out\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\CSSoftware\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CSSoftware\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 812\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    815\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    816\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "node_csv = pd.read_csv('node_test_4_A.csv')\n",
    "edge_csv = pd.read_csv('edge_test_4_A.csv')\n",
    "geohash_id = node_csv['geohash_id']\n",
    "date_id = node_csv['date_id']\n",
    "geohasd_df_dict2, date_df_dict2, x_data2, x_mask2, x_edge_df2 = get_test_data('node_test_4_A.csv','edge_test_4_A.csv')#得到测试集数据\n",
    "x_data2 = torch.tensor(x_data2[:])\n",
    "x_mask2 = torch.tensor(x_mask2[:])\n",
    "x_edge_df2 = torch.tensor(x_edge_df2[:])\n",
    "outset=DataIterator2 (x_data2,x_mask2,x_edge_df2, args)\n",
    "predict = torch.Tensor()\n",
    "model1 = torch.load('best_model_3861.89404296875.pth')\n",
    "with torch.no_grad():\n",
    "    act_pre, con_pre = model1(outset.x_date,outset.x_feature,outset.x_mask_data)\n",
    "    print(act_pre.shape)\n",
    "    print(con_pre.shape)\n",
    "    act_pre=act_pre.reshape(1,4560)\n",
    "    con_pre=con_pre.reshape(1,4560)\n",
    "    predict = torch.cat((act_pre, con_pre), dim= 0)\n",
    "        \n",
    "\n",
    "consumption_level = predict[1,:].to('cpu')\n",
    "activity_level = predict[0, :].to('cpu')\n",
    "print(consumption_level.shape)\n",
    "print(activity_level.shape)\n",
    "\n",
    "output = {\n",
    "    'geohash_id': geohash_id.tolist(),\n",
    "    'consumption_level': consumption_level.tolist(),\n",
    "    'activity_level': activity_level.tolist(),\n",
    "    'date_id': date_id.tolist()\n",
    "}\n",
    "df = pd.DataFrame.from_dict(output)\n",
    "df.to_csv('output2.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e715d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d862bbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc18ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f20c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a48bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec64419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd2bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629cc182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea5436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b68b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048b389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6ba3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2861b45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6a48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01f97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20186a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5956969c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e3e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaea038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfffd4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32589ff7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# model1 = torch.load('best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81579b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GAT(date_emb =[124,6], nfeat=35, nhid=64, dropout=0.3, alpha=0.3, nheads=8).to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd979c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(),'my_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = torch.load('my_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a4056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kk = model.load_state_dict(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b40222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# # 定义一个类似 argparse 的命名元组\n",
    "# from collections import namedtuple\n",
    "#\n",
    "#\n",
    "# Args = namedtuple('Args', ['epochs', 'batch_size', 'device', 'lr', 'rat', 'decline'])\n",
    "#\n",
    "# # 创建一个 Args 对象并设置各个参数的值\n",
    "# args = Args(300, 4, 'cuda', 1e-3, 0.9, 30)\n",
    "# \"\"\"\n",
    "# geohasd_df_dict, date_df_dict, x_train, x_mask, x_edge_df = get_train_data('./train_90.csv',\"./edge_90.csv\")\n",
    "# #分割各种训练集测试集\n",
    "# x_train,x_dev = torch.tensor(x_train[:int(len(x_train)*args.rat)]),torch.tensor(x_train[int(len(x_train)*args.rat):])\n",
    "# x_mask_train,x_mask_dev = torch.tensor(x_mask[:int(len(x_mask)*args.rat)]),torch.tensor(x_mask[int(len(x_mask)*args.rat):])\n",
    "# x_edge_train, x_edge_dev = torch.tensor(x_edge_df[:int(len(x_edge_df) * args.rat)]),torch.tensor( x_edge_df[int(len(x_edge_df) * args.rat):])\n",
    "#\n",
    "# date_emb = 5\n",
    "#\n",
    "#\n",
    "#     # 这里的x包含了date_id+F35个特征+2个y值的\n",
    "# # train_activate = torch.tensor(y_train[:, -2])\n",
    "# # train_consume = torch.tensor(y_train[:, -1])\n",
    "#\n",
    "# model = GAT(date_emb =[len(date_df_dict),date_emb], nfeat=35, nhid=64, dropout=0.3, alpha=0.3, nheads=8)\n",
    "# def train(args,model):\n",
    "#\n",
    "#     model = model.to(args.device)\n",
    "#     # rmse_loss = torch.sqrt(mse_loss)\n",
    "#\n",
    "#     # model = my_model.BILSTM(date_emb =[len(date_df_dict),date_emb], nfeat=35, nhid=64, dropout=0.3, alpha=0.3, nheads=8).to(args.device)\n",
    "#     optimizer = torch.optim.Adam(params=model.parameters(),lr=args.lr)\n",
    "#     # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.decline, gamma=0.5, last_epoch=-1)\n",
    "#     model.train()\n",
    "#     trainset = DataIterator(x_train,x_mask_train,x_edge_train, args)\n",
    "#     valset =DataIterator(x_dev,x_mask_dev,x_edge_dev, args)\n",
    "#     for indx in range(args.epochs):\n",
    "#         train_all_loss = 0.0\n",
    "#         for j in trange(trainset.batch_count):\n",
    "#             x_date,x_feature,x_mask_data,x_edge_data,x_tags= trainset.get_batch(j)\n",
    "#             act_pre, con_pre = model(x_date,x_feature,x_mask_data)#!!!!!\n",
    "#             predict = torch.cat((act_pre, con_pre), dim=-1)\n",
    "#\n",
    "#             loss = criterion(predict, x_tags)\n",
    "#             train_all_loss += loss\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#         print('this epoch train loss :{0}'.format(train_all_loss/trainset.batch_count))\n",
    "#         # scheduler.step()\n",
    "#         eval(model,valset, args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
